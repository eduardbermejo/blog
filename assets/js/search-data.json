{
  
    
        "post0": {
            "title": "Shannon's entropy",
            "content": "In this post, I am going to write about Shannon&#39;s information, entropy, cross-entropy and the Kullback-Leibler divergence. With the last two concepts being heavily used in machine learning I think it is nice to have a good understanding of the meaning and relationship of these terms. . So to try to explain these concepts I would like to first post a question/exercice: . Suppose we have 4 letters A, B, C and D in some alphabet we will use to communicate between each other through some channel. I tell you that I will be sending you a messsage every morning. This message will have no meaning, it will be just a concatenation of 2 out of these 4 letters picked randomly with equal probability. . Let&#39;s generate some messages: . import numpy as np . abecedary = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;] p = np.array([1/len(abecedary)] * len(abecedary)) np.random.choice(abecedary, size=(10,2), p=p) . array([[&#39;B&#39;, &#39;D&#39;], [&#39;A&#39;, &#39;B&#39;], [&#39;A&#39;, &#39;B&#39;], [&#39;B&#39;, &#39;B&#39;], [&#39;B&#39;, &#39;C&#39;], [&#39;D&#39;, &#39;C&#39;], [&#39;D&#39;, &#39;D&#39;], [&#39;D&#39;, &#39;A&#39;], [&#39;B&#39;, &#39;C&#39;], [&#39;A&#39;, &#39;B&#39;]], dtype=&#39;&lt;U1&#39;) . So for 10 consecutive days I send you one of these messages every day. So far we have talked about the message but not about the channel we use to transmit it. So let me tell you that this channel will be digital meaning we will use values of 1 or 0 to encode our message. . The first question is: how many bits do I need to encode my message? Well, we know that if I have 4 letters, and each letter is equally likely, I am gonna need $log_2(4)=2$ bits to encode each letter. And since we know that each letter is independent of each otherwe will need 2*2 bits to encode the message I&#39;m sending to you every day. The key thing to note here is that these 2 bits required for each letter is actually the information each letter in the message is carrying. Bear with me. . Let&#39;s look at another example, a more interesting one: Our variables will be independent as in the previous example but this time the probability mass function won&#39;t be uniform. Our variables will follow this distribution $p=(0.7, 0.1, 0.1, 0.1)$. Let&#39;s generate the 10 messages again: . p = np.array([0.7, 0.1, 0.1, 0.1]) np.random.choice(abecedary, size=(10,2), p=p) . array([[&#39;A&#39;, &#39;A&#39;], [&#39;B&#39;, &#39;B&#39;], [&#39;D&#39;, &#39;C&#39;], [&#39;A&#39;, &#39;D&#39;], [&#39;B&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;], [&#39;C&#39;, &#39;C&#39;], [&#39;A&#39;, &#39;B&#39;], [&#39;D&#39;, &#39;A&#39;], [&#39;B&#39;, &#39;A&#39;]], dtype=&#39;&lt;U1&#39;) . By seeing these samples, I&#39;m asking you again how should we encode our messages? One could say that we still have 4 letters, threfore we could use the same encoding as before. This way, we would be using again 2 bits for each letter. But if you think carefully, we know that &#39;A&#39; is far more likely than the 3 other letters. So how about we take advantage of this information to encode our message? Maybe we can use one bit to encode whether the letter is A or not. And then use two more bits for the rest of the letters. . This waw we would have for example the following codes for each letter: . A -&gt; 1 B -&gt; 01 C -&gt; 001 D -&gt; 000 . It is true that we now need 3 bits for some letters. But the key thing to mesure is the expected number of bits to be used for each letter. Let&#39;s compute it: . num_bits = np.array([1, 2, 3, 3]) np.sum(p * num_bits) . 1.5 . Spoiler alert: this is close to the entropy of our message, also refered to as the expected information of our message, but it is actually not. It is actually the cross-entropy. Bear with me again. . Information . So we have managed to make our encoding more efficient by using the distribution of our random process of sending messages. Going back to the information concept, here we would have: . $-log_2(0.7)=0.51$ bits for A, . $-log_2(0.1)=3.32$ bits for B, . $-log_2(0.1)=3.32$ bits for C, . $-log_2(0.1)=3.32$ bits for D, . Wait, where does this formula come from? Well let me tell you that the way we compute information is this: . $$ I_X(x)=-log_2(p(x)) $$ . We could actually use another base, but let&#39;s stick to base two and continue talking about bits. Try to think of information as the surprise you have after receiving the message. So A wouldn&#39;t surprise you that much, but B, C and D would, therefore they carry more information. The other way to think about it is the number of bits we need to represent each value in the most optimal way. . So, as we computed previously we would need 0.51 bits to encode A, and 3.32 bits to encode the rest of the letters. Going back to the previous example, let us try to compute information in a different way to make it more intuitive (hopefully): . Let&#39;s transform our probabilities in frequency counts like so: We now will have in our alphabet 7 As, 1 B, 1 C and 1 D. We then have now 10 letters. So what is the amount of bits we need to encode one letter? It is actually $log_2(10)=3.32$. So the letters only appearing once will need this amount of bits to be encoded. As you can see it is the same result we got above. How about A? Well, we know that A is 7 times more likely than B, C, or D. So the way to compute it is $log_2( frac{10}{7})=0.51$. Intuitively, out of the ten buckets, A is filling 7. . As you can see, for the case of B, C and D we used $log_2{10}$ which is actually $-log_2 frac{1}{10}$ and the same goes for A which becomes $-log_2( frac{7}{10})$ which leads us to the formal definition presented before: $I_X(x)=-log_2(p(x))$. This should solve the first &quot;bear with me&quot;. . Entropy . Let&#39;s go to the second one. Since we now know what information in this context means and how we can quantify the amount each event carries, let&#39;s calculate the expected amount of information in our message like this: . $$ large E[I_X]=H(p)= sum_i p_X(x_i)I_X(x_i) $$ &quot;The entropy of our source or the expected information in our message is {}&quot;.format(-np.sum(p*np.log2(p))) . &#39;The entropy of our source or the expected information in our message is 1.3567796494470397&#39; . Previously we tried to adjust the encoding of our messages by assigning 1 bit to A, two bits to B and C and three bits to D. That reduced the amount of bits used on average to send a message but we still are using more bits than required. Our message has an entropy of 1.35 and we use 1.5 bits to encode it. . We can not do better for four letters using bits but now we know that we are using a code which could carry more information than what our &quot;source&quot; or message is actually carrying. So this concept of expected information is known as entropy. . And this attemp we have made to find an optimal code will lead us to the concept of cross-entropy. . Cross-entropy . When we tried to use our improved encoding, we were assuming a probability mass function for our source: . $P(A)=2^{-1}$ . $P(B)=2^{-2}$ . $P(C)=2^{-3}$ . $P(D)=2^{-3}$ . &quot;This the probability distribution we were assuming by using our improved encoding: {}&quot;.format(2**(-num_bits).astype(float)) . &#39;This the probability distribution we were assuming by using our improved encoding: [0.5 0.25 0.125 0.125]&#39; . So when we instead of plugging in the source distribution in the information term of the entropy fromula use another distribution, we are getting what we call cross-entropy. And this gives us the expected length in bits of our source messages in each letter. The idea is that we know the distribution of our alphabet and the encoding or pmf we are using to transmit it. So the formula for the cross-entropy $H(p,q)$ is the same as the one for the entropy, only that we use now two different probability distributions: . $$ large H(p,q)=- sum_i p(x_i)log(q(x_i)) $$ Kullback-Leibler divergence . So we now know the entropy or expected information in each of our letter&#39;s message, and the cross-entropy or expected length of each letter in our message. And this difference here between the optimal encoding or information and the expected length of our encoding is what is called kullback-Leibler divergence: . $$ large D_{KL}(p||q)=H(p,q)-H(p)=- sum_i p(x_i)log(q(x_i))+ sum_i p(x_i)log(p(x_i))= sum_i p(x_i)log( frac{p(x_i)}{q(x_i}) $$ properties and uses of the kl divergence | cross-entropy in ML -&gt; maximum likelihood estimation | .",
            "url": "https://eduardbermejo.github.io/blog/2020/05/12/Shannon's_Entropy.html",
            "relUrl": "/2020/05/12/Shannon's_Entropy.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://eduardbermejo.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://eduardbermejo.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://eduardbermejo.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://eduardbermejo.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}